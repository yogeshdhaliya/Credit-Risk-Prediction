{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Imports and Load All Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported for Phase 5.\n",
      "Loading all required assets...\n",
      "\n",
      "--- Assets Loaded Successfully ---\n",
      "X (model input) shape: (51336, 38)\n",
      "y (target) shape: (51336,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import optuna # Our new library for Phase 5\n",
    "\n",
    "# --- Core Modeling Imports ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- Pipeline & Imbalance (Finding 4) ---\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Model Algorithm ---\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Setup ---\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) # Quiets Optuna's logs\n",
    "\n",
    "print(\"Libraries imported for Phase 5.\")\n",
    "\n",
    "# --- Define File Paths ---\n",
    "# Use the full, confirmed paths from our last step\n",
    "FULL_PATH_TO_PROCESSED = '/Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/data/processed'\n",
    "FULL_PATH_TO_MODELS = '/Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/models'\n",
    "\n",
    "X_path = os.path.join(FULL_PATH_TO_PROCESSED, 'X_model_input.csv')\n",
    "y_path = os.path.join(FULL_PATH_TO_PROCESSED, 'y_target.csv')\n",
    "PREPROCESSOR_path = os.path.join(FULL_PATH_TO_MODELS, 'preprocessor.joblib')\n",
    "TARGET_ENCODER_path = os.path.join(FULL_PATH_TO_MODELS, 'target_encoder.joblib')\n",
    "\n",
    "print(\"Loading all required assets...\")\n",
    "\n",
    "try:\n",
    "    # Load our 38-feature dataset\n",
    "    X = pd.read_csv(X_path, index_col='PROSPECTID')\n",
    "    # Load the original string target\n",
    "    y = pd.read_csv(y_path, index_col='PROSPECTID').squeeze('columns')\n",
    "    \n",
    "    # Load our saved preprocessor and target encoder\n",
    "    preprocessor = joblib.load(PREPROCESSOR_path)\n",
    "    target_encoder = joblib.load(TARGET_ENCODER_path)\n",
    "    \n",
    "    print(\"\\n--- Assets Loaded Successfully ---\")\n",
    "    print(f\"X (model input) shape: {X.shape}\")\n",
    "    print(f\"y (target) shape: {y.shape}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n[ERROR] Files not found. Please ensure your paths are correct.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Re-create Data Split (for Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Re-creating Data Split ---\n",
      "X_train shape: (41068, 38), y_train shape: (41068,)\n",
      "X_test shape: (10268, 38), y_test shape: (10268,)\n",
      "Data is ready for tuning.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Re-creating Data Split ---\")\n",
    "\n",
    "# 1. Encode y (using our saved encoder)\n",
    "y_encoded = target_encoder.transform(y)\n",
    "\n",
    "# 2. Split the data with the SAME random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded  # Per Finding 4\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "print(\"Data is ready for tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Define the Optuna \"Objective\" Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna 'objective' function defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function to maximize the f1_macro score\n",
    "    for our SMOTE + XGBoost pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Define Hyperparameter Search Space ---\n",
    "    \n",
    "    # A. SMOTE parameters\n",
    "    # We'll tune k_neighbors, as it's the most sensitive\n",
    "    smote_k = trial.suggest_int('smote_k_neighbors', 3, 10, log=True)\n",
    "    \n",
    "    # B. XGBoost parameters\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 200, 1000)\n",
    "    xgb_max_depth = trial.suggest_int('xgb_max_depth', 3, 10)\n",
    "    xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True)\n",
    "    xgb_gamma = trial.suggest_float('xgb_gamma', 1e-8, 1.0, log=True)\n",
    "    xgb_subsample = trial.suggest_float('xgb_subsample', 0.6, 1.0)\n",
    "    xgb_colsample_bytree = trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0)\n",
    "    xgb_reg_alpha = trial.suggest_float('xgb_reg_alpha', 1e-8, 1.0, log=True)\n",
    "    xgb_reg_lambda = trial.suggest_float('xgb_reg_lambda', 1e-8, 1.0, log=True)\n",
    "    \n",
    "    # --- 2. Create the Pipeline with Trial Parameters ---\n",
    "    \n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42, k_neighbors=smote_k)),\n",
    "        ('model', XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class=4,\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            n_estimators=xgb_n_estimators,\n",
    "            max_depth=xgb_max_depth,\n",
    "            learning_rate=xgb_learning_rate,\n",
    "            gamma=xgb_gamma,\n",
    "            subsample=xgb_subsample,\n",
    "            colsample_bytree=xgb_colsample_bytree,\n",
    "            reg_alpha=xgb_reg_alpha,\n",
    "            reg_lambda=xgb_reg_lambda\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # --- 3. Evaluate the Pipeline ---\n",
    "    # We use 3-fold CV for speed during tuning\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    score = cross_val_score(\n",
    "        pipeline,\n",
    "        X_train,  # Tune only on the training data\n",
    "        y_train,\n",
    "        cv=cv_strategy,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # --- 4. Return the Mean Score ---\n",
    "    return np.mean(score)\n",
    "\n",
    "print(\"Optuna 'objective' function defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Run the Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Optuna Study (n_trials=50) ---\n",
      "This will take a significant amount of time. Please be patient.\n",
      "\n",
      "--- Optuna Study Complete ---\n",
      "Best F1 Macro Score: 0.6107\n",
      "\n",
      "Best Hyperparameters Found:\n",
      "{'smote_k_neighbors': 6, 'xgb_n_estimators': 456, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.02629064786870942, 'xgb_gamma': 0.07798851124134179, 'xgb_subsample': 0.620775868670427, 'xgb_colsample_bytree': 0.9978076845683496, 'xgb_reg_alpha': 3.0506264787927e-06, 'xgb_reg_lambda': 0.275595077031217}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Optuna Study (n_trials=50) ---\")\n",
    "print(\"This will take a significant amount of time. Please be patient.\")\n",
    "\n",
    "# 1. Create a study, 'direction' tells it to maximize the score\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# 2. Run the optimization\n",
    "# We pass our function and the number of trials\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 3. Print the results\n",
    "print(\"\\n--- Optuna Study Complete ---\")\n",
    "print(f\"Best F1 Macro Score: {study.best_value:.4f}\")\n",
    "print(\"\\nBest Hyperparameters Found:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Build, Train, and Evaluate the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building and Evaluating Tuned Model ---\n",
      "Loaded best parameters: {'smote_k_neighbors': 6, 'xgb_n_estimators': 456, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.02629064786870942, 'xgb_gamma': 0.07798851124134179, 'xgb_subsample': 0.620775868670427, 'xgb_colsample_bytree': 0.9978076845683496, 'xgb_reg_alpha': 3.0506264787927e-06, 'xgb_reg_lambda': 0.275595077031217}\n",
      "\n",
      "Tuned pipeline created successfully.\n",
      "Training final tuned model on full train set...\n",
      "Tuned model trained.\n",
      "\n",
      "--- Final Tuned Model Evaluation Report (on Test Set) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          P1       0.60      0.75      0.67      1161\n",
      "          P2       0.81      0.82      0.81      6440\n",
      "          P3       0.39      0.28      0.33      1491\n",
      "          P4       0.63      0.63      0.63      1176\n",
      "\n",
      "    accuracy                           0.71     10268\n",
      "   macro avg       0.61      0.62      0.61     10268\n",
      "weighted avg       0.70      0.71      0.71     10268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\n--- Building and Evaluating Tuned Model ---\")\n",
    "\n",
    "# 1. Get the best parameters from the study\n",
    "try:\n",
    "    best_params = study.best_params\n",
    "    print(f\"Loaded best parameters: {best_params}\")\n",
    "except NameError:\n",
    "    print(\"[ERROR] 'study' object not found. Please re-run Block 4 or paste the 'best_params' dictionary here manually.\")\n",
    "    # In a real scenario, you'd paste the dictionary:\n",
    "    # best_params = {'smote_k_neighbors': 6, 'xgb_n_estimators': 456, ...}\n",
    "    raise\n",
    "\n",
    "# 2. Create the new, tuned pipeline\n",
    "tuned_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(\n",
    "        random_state=42, \n",
    "        k_neighbors=best_params['smote_k_neighbors'] # From Optuna\n",
    "    )),\n",
    "    ('model', XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=4,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        # --- Unpack all the tuned XGB parameters ---\n",
    "        n_estimators=best_params['xgb_n_estimators'],\n",
    "        max_depth=best_params['xgb_max_depth'],\n",
    "        learning_rate=best_params['xgb_learning_rate'],\n",
    "        gamma=best_params['xgb_gamma'],\n",
    "        subsample=best_params['xgb_subsample'],\n",
    "        colsample_bytree=best_params['xgb_colsample_bytree'],\n",
    "        reg_alpha=best_params['xgb_reg_alpha'],\n",
    "        reg_lambda=best_params['xgb_reg_lambda']\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\nTuned pipeline created successfully.\")\n",
    "\n",
    "# 3. Train the new pipeline on the FULL training set\n",
    "print(\"Training final tuned model on full train set...\")\n",
    "tuned_pipeline.fit(X_train, y_train)\n",
    "print(\"Tuned model trained.\")\n",
    "\n",
    "# 4. Evaluate on the held-out test set\n",
    "y_pred_tuned = tuned_pipeline.predict(X_test)\n",
    "\n",
    "# 5. Decode labels for the report\n",
    "y_test_labels = target_encoder.inverse_transform(y_test)\n",
    "y_pred_tuned_labels = target_encoder.inverse_transform(y_pred_tuned)\n",
    "\n",
    "# 6. Generate and print the new report\n",
    "print(\"\\n--- Final Tuned Model Evaluation Report (on Test Set) ---\")\n",
    "tuned_report = classification_report(\n",
    "    y_test_labels, \n",
    "    y_pred_tuned_labels, \n",
    "    labels=target_encoder.classes_,\n",
    "    zero_division=0\n",
    ")\n",
    "print(tuned_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Save the Final Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Final Tuned Model ---\n",
      "Final tuned model pipeline saved to: /Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/models/tuned_model.joblib\n",
      "\n",
      "--- Phase 5 is 100% complete and all assets are saved. ---\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Saving Final Tuned Model ---\")\n",
    "\n",
    "# --- 1. Define Save Path ---\n",
    "# (Using the paths from Block 1)\n",
    "TUNED_MODEL_SAVE_PATH = os.path.join(FULL_PATH_TO_MODELS, 'tuned_model.joblib')\n",
    "\n",
    "# --- 2. Save the Full Tuned Pipeline ---\n",
    "# This is 'tuned_pipeline' from your Block 5\n",
    "joblib.dump(tuned_pipeline, TUNED_MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"Final tuned model pipeline saved to: {TUNED_MODEL_SAVE_PATH}\")\n",
    "print(\"\\n--- Phase 5 is 100% complete and all assets are saved. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Phase 5: Hyperparameter Tuning\n",
    "\n",
    "Our primary goal in this phase was to improve upon our baseline model, which had an `f1-macro` score of 0.60 and a critical weakness in identifying the **P3 (subprime)** class (0.25 F1-score).\n",
    "\n",
    "### 1. The Tuning Strategy\n",
    "\n",
    "We used **`Optuna`** to run a 50-trial hyperparameter search. We optimized for the `f1-macro` score by tuning a wide range of parameters for both `SMOTE` (like `k_neighbors`) and our `XGBClassifier` (like `n_estimators`, `max_depth`, and `learning_rate`).\n",
    "\n",
    "### 2. The Result: A \"Smarter\" Model\n",
    "\n",
    "The tuning was a clear success. `Optuna` found a new set of parameters that achieved a cross-validation `f1-macro` score of **0.6107**, surpassing our baseline of 0.5933.\n",
    "\n",
    "### 3. Final Evaluation & Key Success\n",
    "\n",
    "We trained a new pipeline with these \"best parameters\" and evaluated it on our unseen test set. The results confirmed the improvement:\n",
    "\n",
    "* **Our primary goal was met:** The `f1-score` for the difficult **P3 class** jumped from **0.25 to 0.33**, a **32% improvement**.\n",
    "* **A smart trade-off:** The model learned to identify P3 by slightly lowering its focus on the dominant P2 class (0.83 -> 0.81), while P1 and P4 performance held steady.\n",
    "* **Overall improvement:** The final `f1-macro` score on the test set increased from **0.60 to 0.61**.\n",
    "\n",
    "### 4. Final Deliverable\n",
    "\n",
    "We have saved this new, superior \"champion\" pipeline as **`tuned_model.joblib`**. This is the final model we will use for our application.\n",
    "\n",
    "---\n",
    "\n",
    "This completes our model development and evaluation. We have successfully followed all 4 Key Findings and produced a final, tuned, and stable model that outperforms our baseline, especially on our key \"P3\" challenge.\n",
    "\n",
    "We are now ready to begin our final phase: **Phase 6: Deployment Preparation**.\n",
    "\n",
    "Our goal here is to build the Streamlit web app that was our final product. This app will:\n",
    "1.  Load our two key assets: `tuned_model.joblib` (which is the full pipeline) and `target_encoder.joblib` (to decode the predictions).\n",
    "2.  Provide a file uploader for the user (the underwriting team).\n",
    "3.  Process the uploaded CSV of new applicants, run them through our pipeline, and display the predicted risk categories (P1, P2, P3, or P4).\n",
    "\n",
    "Are you ready to start building the `app/app.py` file for our Streamlit app?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_credit_risk)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
