{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Imports and Load Final Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported for Phase 4.\n",
      "Loading X data from: /Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/data/processed/X_model_input.csv\n",
      "Loading y data from: /Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/data/processed/y_target.csv\n",
      "Loading preprocessor from: /Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/models/preprocessor.joblib\n",
      "\n",
      "--- Assets Loaded Successfully ---\n",
      "X (model input) shape: (51336, 38)\n",
      "y (target) shape: (51336,)\n",
      "\n",
      "Preprocessor object:\n",
      "ColumnTransformer(remainder='passthrough',\n",
      "                  transformers=[('ordinal',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('encoder',\n",
      "                                                  OrdinalEncoder(categories=[['OTHERS',\n",
      "                                                                              'SSC',\n",
      "                                                                              '12TH',\n",
      "                                                                              'UNDER '\n",
      "                                                                              'GRADUATE',\n",
      "                                                                              'GRADUATE',\n",
      "                                                                              'POST-GRADUATE',\n",
      "                                                                              'PROFESSIONAL']],\n",
      "                                                                 handle_unknown='use_encoded_value',\n",
      "                                                                 unknown_value=-1))]),\n",
      "                                 ['EDUCATION']),\n",
      "                                ('nominal',\n",
      "                                 Pipeline(s...\n",
      "                                 ['AGE', 'NETMONTHLYINCOME',\n",
      "                                  'Time_With_Curr_Empr',\n",
      "                                  'pct_opened_TLs_L6m_of_L12m', 'CC_Flag',\n",
      "                                  'PL_Flag', 'HL_Flag', 'GL_Flag',\n",
      "                                  'Total_TL_opened_L6M', 'Tot_TL_closed_L6M',\n",
      "                                  'pct_tl_open_L6M', 'pct_tl_closed_L6M',\n",
      "                                  'Total_TL_opened_L12M', 'Tot_TL_closed_L12M',\n",
      "                                  'pct_tl_open_L12M', 'pct_tl_closed_L12M',\n",
      "                                  'Tot_Missed_Pmnt', 'Auto_TL', 'CC_TL',\n",
      "                                  'Consumer_TL', 'Gold_TL', 'Home_TL', 'PL_TL',\n",
      "                                  'Other_TL'])])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "# --- Core Modeling Imports ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# --- Pipeline & Imbalance (Finding 4) ---\n",
    "# We MUST use the pipeline from imblearn to correctly handle SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Model Algorithm ---\n",
    "# We'll use XGBoost as our powerful baseline, as defined in requirements.txt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Setup ---\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries imported for Phase 4.\")\n",
    "\n",
    "# --- Define File Paths ---\n",
    "# Use the full, confirmed paths from our last step\n",
    "FULL_PATH_TO_PROCESSED = '/Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/data/processed'\n",
    "FULL_PATH_TO_MODELS = '/Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/models'\n",
    "\n",
    "X_path = os.path.join(FULL_PATH_TO_PROCESSED, 'X_model_input.csv')\n",
    "y_path = os.path.join(FULL_PATH_TO_PROCESSED, 'y_target.csv') # We load the original y\n",
    "PREPROCESSOR_path = os.path.join(FULL_PATH_TO_MODELS, 'preprocessor.joblib')\n",
    "\n",
    "print(f\"Loading X data from: {X_path}\")\n",
    "print(f\"Loading y data from: {y_path}\")\n",
    "print(f\"Loading preprocessor from: {PREPROCESSOR_path}\")\n",
    "\n",
    "try:\n",
    "    # Load our 38-feature dataset\n",
    "    X = pd.read_csv(X_path, index_col='PROSPECTID')\n",
    "    \n",
    "    # Load the original target with P1, P2, P3, P4\n",
    "    y = pd.read_csv(y_path, index_col='PROSPECTID').squeeze('columns')\n",
    "    \n",
    "    # Load our saved preprocessor from Phase 3\n",
    "    preprocessor = joblib.load(PREPROCESSOR_path)\n",
    "    \n",
    "    print(\"\\n--- Assets Loaded Successfully ---\")\n",
    "    print(f\"X (model input) shape: {X.shape}\")\n",
    "    print(f\"y (target) shape: {y.shape}\")\n",
    "    print(\"\\nPreprocessor object:\")\n",
    "    print(preprocessor)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n[ERROR] Files not found. Please ensure your paths are correct.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Encode Target Variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Encoding Target Variable (y) ---\n",
      "Target variable encoded.\n",
      "Original classes: ['P1' 'P2' 'P3' 'P4']\n",
      "Encoded classes: [0 1 2 3]\n",
      "Mapping: {'P1': 0, 'P2': 1, 'P3': 2, 'P4': 3}\n",
      "\n",
      "Target encoder saved to: /Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/models/target_encoder.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Encoding Target Variable (y) ---\")\n",
    "\n",
    "# 1. Instantiate the LabelEncoder\n",
    "target_encoder = LabelEncoder()\n",
    "\n",
    "# 2. Fit and transform y\n",
    "y_encoded = target_encoder.fit_transform(y)\n",
    "\n",
    "# 3. Show the mapping (our \"decoder ring\")\n",
    "print(\"Target variable encoded.\")\n",
    "print(f\"Original classes: {target_encoder.classes_}\")\n",
    "print(f\"Encoded classes: {np.unique(y_encoded)}\")\n",
    "print(f\"Mapping: {dict(zip(target_encoder.classes_, np.unique(y_encoded)))}\")\n",
    "\n",
    "# 4. CRITICAL: Save this encoder for our app\n",
    "ENCODER_SAVE_PATH = os.path.join(FULL_PATH_TO_MODELS, 'target_encoder.joblib')\n",
    "joblib.dump(target_encoder, ENCODER_SAVE_PATH)\n",
    "print(f\"\\nTarget encoder saved to: {ENCODER_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Train-Test Split (Finding 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting Data (Stratified) ---\n",
      "X_train shape: (41068, 38), y_train shape: (41068,)\n",
      "X_test shape: (10268, 38), y_test shape: (10268,)\n",
      "\n",
      "Train set distribution (P1, P2, P3, P4): [0.11303204 0.62722801 0.14514951 0.11459044]\n",
      "Test set distribution (P1, P2, P3, P4): [0.11306973 0.62719127 0.14520841 0.11453058]\n",
      "=> CONFIRMED: Distributions are nearly identical.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Splitting Data (Stratified) ---\")\n",
    "\n",
    "# We split the 38-feature X and the new 0,1,2,3 encoded y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded  # Per Finding 4\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Verify stratification worked\n",
    "train_counts = np.bincount(y_train)\n",
    "test_counts = np.bincount(y_test)\n",
    "\n",
    "print(f\"\\nTrain set distribution (P1, P2, P3, P4): {train_counts / len(y_train)}\")\n",
    "print(f\"Test set distribution (P1, P2, P3, P4): {test_counts / len(y_test)}\")\n",
    "print(\"=> CONFIRMED: Distributions are nearly identical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Build Full Modeling Pipeline (Finding 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Full Modeling Pipeline (with SMOTE) ---\n",
      "Full model pipeline created successfully:\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('ordinal',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('encoder',\n",
      "                                                                   OrdinalEncoder(categories=[['OTHERS',\n",
      "                                                                                               'SSC',\n",
      "                                                                                               '12TH',\n",
      "                                                                                               'UNDER '\n",
      "                                                                                               'GRADUATE',\n",
      "                                                                                               'GRADUATE',\n",
      "                                                                                               'POST-GRADUATE',\n",
      "                                                                                               'PROFESSIONAL']],\n",
      "                                                                                  handle_unknown='use_encoded_value',\n",
      "                                                                                  unknown_value=-1))]),\n",
      "                                                  ['ED...\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=-1, num_class=4,\n",
      "                               num_parallel_tree=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Building Full Modeling Pipeline (with SMOTE) ---\")\n",
    "\n",
    "# --- 1. Define SMOTE (Finding 4) ---\n",
    "# We set k_neighbors=4 because our minority classes are small.\n",
    "# random_state=42 ensures reproducibility.\n",
    "smote = SMOTE(random_state=42, k_neighbors=4)\n",
    "\n",
    "# --- 2. Define Model ---\n",
    "# We use XGBoost as our powerful baseline.\n",
    "# 'objective='multi:softmax'' is for multiclass classification.\n",
    "# 'num_class=4' explicitly tells it we have 4 target classes (0,1,2,3).\n",
    "# 'enable_categorical=True' is a modern XGBoost feature, but we'll\n",
    "# let our preprocessor handle encoding for robustness.\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=4,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# --- 3. Create the Full Pipeline ---\n",
    "# This is the key: we use ImbPipeline\n",
    "model_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', smote),\n",
    "    ('model', xgb_model)\n",
    "])\n",
    "\n",
    "print(\"Full model pipeline created successfully:\")\n",
    "print(model_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Cross-Validate the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running 5-Fold Stratified Cross-Validation ---\n",
      "This may take a few minutes...\n",
      "\n",
      "--- Cross-Validation Results ---\n",
      "F1 Macro Scores for each fold: [0.59445626 0.59295003 0.58830453 0.59718419 0.59341853]\n",
      "Mean F1 Macro Score: 0.5933\n",
      "Std Dev of F1 Macro Score: 0.0029\n",
      "\n",
      "=> This 'Mean F1 Macro Score' is our robust baseline for model performance.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Running 5-Fold Stratified Cross-Validation ---\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# 1. Define our cross-validation strategy\n",
    "# 5 splits, stratified, with shuffling for randomness.\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 2. Run the cross-validation\n",
    "# We score on 'f1_macro' as per our project goals\n",
    "# We pass the full X_train (unprocessed) and y_train\n",
    "# The pipeline handles all preprocessing and SMOTE internally.\n",
    "cv_scores = cross_val_score(\n",
    "    model_pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1 # Use all available cores for CV\n",
    ")\n",
    "\n",
    "print(\"\\n--- Cross-Validation Results ---\")\n",
    "print(f\"F1 Macro Scores for each fold: {cv_scores}\")\n",
    "print(f\"Mean F1 Macro Score: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Std Dev of F1 Macro Score: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "print(\"\\n=> This 'Mean F1 Macro Score' is our robust baseline for model performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Train Final Model & Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Model on Full Train Set ---\n",
      "Final model trained successfully.\n",
      "\n",
      "--- Final Test Set Evaluation Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          P1       0.68      0.67      0.67      1161\n",
      "          P2       0.78      0.89      0.83      6440\n",
      "          P3       0.40      0.19      0.25      1491\n",
      "          P4       0.67      0.59      0.63      1176\n",
      "\n",
      "    accuracy                           0.73     10268\n",
      "   macro avg       0.63      0.59      0.60     10268\n",
      "weighted avg       0.70      0.73      0.71     10268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training Final Model on Full Train Set ---\")\n",
    "# This will fit the preprocessor, then fit SMOTE, then fit the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Final model trained successfully.\")\n",
    "\n",
    "# --- 1. Make Predictions on Test Set ---\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# --- 2. Decode Predictions ---\n",
    "# We use our saved encoder to turn 0,1,2,3 back to P1,P2,P3,P4\n",
    "y_test_labels = target_encoder.inverse_transform(y_test)\n",
    "y_pred_labels = target_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# --- 3. Generate Final Report ---\n",
    "print(\"\\n--- Final Test Set Evaluation Report ---\")\n",
    "\n",
    "# --- CORRECTION HERE ---\n",
    "# The parameter is 'labels', not 'order'\n",
    "report = classification_report(\n",
    "    y_test_labels, \n",
    "    y_pred_labels, \n",
    "    labels=target_encoder.classes_, # <-- FIX IS HERE\n",
    "    zero_division=0\n",
    ")\n",
    "# --- END CORRECTION ---\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Save the Final Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Final Baseline Model ---\n",
      "Final baseline model pipeline (with SMOTE+XGB) saved to: /Users/yogeshdhaliya/Desktop/DS Learning/11. Projects/Credit-Risk-Prediction/models/baseline_model.joblib\n",
      "\n",
      "--- Phase 4 is 100% complete and all assets are saved. ---\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "print(\"\\n--- Saving Final Baseline Model ---\")\n",
    "\n",
    "# --- 1. Define Save Path ---\n",
    "MODEL_SAVE_PATH = os.path.join(FULL_PATH_TO_MODELS, 'baseline_model.joblib')\n",
    "\n",
    "# --- 2. Save the Full Pipeline ---\n",
    "# This 'model_pipeline' object is the one you just called .fit() on\n",
    "joblib.dump(model_pipeline, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"Final baseline model pipeline (with SMOTE+XGB) saved to: {MODEL_SAVE_PATH}\")\n",
    "print(\"\\n--- Phase 4 is 100% complete and all assets are saved. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Modeling Pipeline & Baseline\n",
    "\n",
    "Our main goal in this phase was to build our first complete, end-to-end model pipeline, integrating our `preprocessor` from Phase 3 with a baseline `XGBClassifier` model. This is where we implemented our core strategy for **Finding 4 (Class Imbalance)**.\n",
    "\n",
    "### 1. Key Actions Performed\n",
    "\n",
    "* **Asset Loading:** We loaded our final 38-feature dataset (`X_model_input.csv`) and our saved `preprocessor.joblib`.\n",
    "* **Target Encoding:** We converted our target variable `y` from strings ('P1', 'P2', 'P3', 'P4') into numerical labels (0, 1, 2, 3) using `LabelEncoder`. We **saved this encoder** (`target_encoder.joblib`) so our future Streamlit app can decode the model's predictions.\n",
    "* **Stratified Split (Finding 4):** We performed our `train_test_split` using `stratify=y_encoded`. This was a non-negotiable step to ensure our unbalanced classes were represented equally in both the train and test sets.\n",
    "* **`ImbPipeline` with `SMOTE` (Finding 4):** We used the `imbalanced-learn` `Pipeline` object to chain our three critical steps:\n",
    "    1.  **`preprocessor`**: Applies all our scaling/encoding rules.\n",
    "    2.  **`SMOTE`**: Resamples the data to fix the class imbalance. Using it inside the pipeline ensures it *only* runs on training data, preventing data leakage.\n",
    "    3.  **`model`**: A baseline `XGBClassifier` to make the predictions.\n",
    "\n",
    "### 2. Key Results & Evaluation\n",
    "\n",
    "* **Robust Baseline Score:** Our 5-fold cross-validation on the training data yielded a very stable and reliable **`f1-macro` score of 0.5933** (with a tiny standard deviation of 0.0029).\n",
    "* **Final Test Report (Qualified Success):**\n",
    "    * The pipeline performed consistently on the unseen test set, achieving an **`f1-macro` of 0.60**.\n",
    "    * **Success (Finding 4):** Our `SMOTE` strategy worked. The model learned to identify the crucial minority classes, achieving good F1-scores for **P1 (0.67)** and **P4 (0.63)**.\n",
    "    * **New Challenge:** The report clearly identified our new primary challenge: the model struggles to distinguish **P3 (subprime)**, which had a very low F1-score of **0.25**.\n",
    "\n",
    "### 3. Final Output\n",
    "\n",
    "* **Final Baseline Model:** We saved our fully trained baseline pipeline (preprocessor + SMOTE + XGB model) as `baseline_model.joblib`. This model is our benchmark to beat in Phase 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_credit_risk)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
